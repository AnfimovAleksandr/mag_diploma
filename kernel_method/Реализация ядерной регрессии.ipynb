{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb48317",
   "metadata": {},
   "source": [
    "# Реализация ядерного сглажиания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf9a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar, minimize\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f46bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6316dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_regression_silverman(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Обучает ядерную регрессию с шириной окна по правилу Сильвермана.\n",
    "    \n",
    "    Возвращает:\n",
    "    KernelReg — обученная модель для предсказания в любых точках\n",
    "    \"\"\"\n",
    "    x_train = np.asarray(x_train).reshape(-1, 1)\n",
    "    h = 1.06 * np.std(x_train) * len(x_train)**(-1/5)\n",
    "    return KernelReg(y_train, x_train, var_type='c', bw=[h])\n",
    "\n",
    "def kernel_regression_cv_safe(x_train, y_train):\n",
    "    x_train = np.asarray(x_train).reshape(-1, 1)\n",
    "    h_silverman = 1.06 * np.std(x_train) * len(x_train)**(-1/5)\n",
    "    \n",
    "    kf = KFold(5)\n",
    "    \n",
    "    def cv_mse(log_h):\n",
    "        h = np.exp(log_h)\n",
    "        # Защита от экстремальных значений\n",
    "        if h < 1e-6 or h > 100.0 or np.isnan(h) or np.isinf(h):\n",
    "            return 1e10\n",
    "        \n",
    "        total_mse = 0.0\n",
    "        try:\n",
    "            for train_idx, val_idx in kf.split(x_train):\n",
    "                kr = KernelReg(y_train[train_idx], x_train[train_idx], var_type='c', bw=[h])\n",
    "                pred = kr.fit(x_train[val_idx])[0]\n",
    "                total_mse += np.mean((pred - y_train[val_idx])**2)\n",
    "            return total_mse / kf.n_splits\n",
    "        except:\n",
    "            return 1e10\n",
    "    \n",
    "    h_init = np.log(h_silverman)\n",
    "    res = minimize(cv_mse, x0=h_init, method='L-BFGS-B')\n",
    "    best_h = np.exp(res.x[0]) if res.success and not (np.isnan(res.x[0]) or np.isinf(res.x[0])) else h_silverman\n",
    "    \n",
    "    return KernelReg(y_train, x_train, var_type='c', bw=[best_h])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b1651",
   "metadata": {},
   "source": [
    "# Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cf70a",
   "metadata": {},
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7a1be",
   "metadata": {},
   "source": [
    "### Интегральные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbaae0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imse(g_hat, g_true, a=0, b=1, epsabs=1e-8):\n",
    "    integrand = lambda x: (g_hat(x) - g_true(x)) ** 2\n",
    "    integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=1000)\n",
    "    return integral / (b - a)\n",
    "\n",
    "def imae(g_hat, g_true, a=0, b=1, epsabs=1e-8):\n",
    "    integrand = lambda x: abs(g_hat(x) - g_true(x))\n",
    "    integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=1000)\n",
    "    return integral / (b - a)\n",
    "\n",
    "def maxerr(g_hat, g_true, a=0, b=1, xatol=1e-8):\n",
    "    objective = lambda x: -abs(g_hat(x) - g_true(x))\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded', options={'xatol': xatol})\n",
    "    if result.success:\n",
    "        return -result.fun\n",
    "    x_grid = np.linspace(a, b, 1000)\n",
    "    return max(abs(g_hat(x) - g_true(x)) for x in x_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7cdc82",
   "metadata": {},
   "source": [
    "### Дискретные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d54b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imse_discrete(g_hat, g_true, a=0, b=1, n_points=1000):\n",
    "    x_grid = np.linspace(a, b, n_points)\n",
    "    errors = [(g_hat(x) - g_true(x)) ** 2 for x in x_grid]\n",
    "    return np.mean(errors)\n",
    "\n",
    "def imae_discrete(g_hat, g_true, a=0, b=1, n_points=1000):\n",
    "    x_grid = np.linspace(a, b, n_points)\n",
    "    errors = [abs(g_hat(x) - g_true(x)) for x in x_grid]\n",
    "    return np.mean(errors)\n",
    "\n",
    "def maxerr_discrete(g_hat, g_true, a=0, b=1, n_points=1000):\n",
    "    x_grid = np.linspace(a, b, n_points)\n",
    "    errors = [abs(g_hat(x) - g_true(x)) for x in x_grid]\n",
    "    return max(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356035b2",
   "metadata": {},
   "source": [
    "## На полиномах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2465f827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:3383: RuntimeWarning: invalid value encountered in matmul\n",
      "  return _core_matmul(x1, x2)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "C:\\Users\\M A C H R E A T O R\\AppData\\Local\\Temp\\ipykernel_16084\\2727826291.py:3: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=1000)\n",
      "C:\\Users\\M A C H R E A T O R\\AppData\\Local\\Temp\\ipykernel_16084\\2727826291.py:8: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=1000)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2236: RuntimeWarning: overflow encountered in divide\n",
      "  s = divide(1, s, where=large, out=s)\n",
      "c:\\Users\\M A C H R E A T O R\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты ядерной регрессии на полиномиальных данных\n",
      "=====================================================================================\n",
      " degree noise_level    method  imse_mean  imse_sem  imae_mean  imae_sem  maxerr_mean  maxerr_sem\n",
      "      1         low silverman   0.160349  0.015640   0.278645  0.007326     0.538294    0.035277\n",
      "      1         low        cv   0.261252  0.130959   0.267028  0.010479     0.666592    0.148567\n",
      "      1    moderate silverman   0.176007  0.029440   0.277048  0.009487     0.531820    0.031539\n",
      "      1    moderate        cv   0.167202  0.034568   0.262182  0.011558     0.511096    0.029212\n",
      "      1        high silverman   0.177551  0.019022   0.282003  0.007961     0.533536    0.036045\n",
      "      1        high        cv   0.174046  0.026942   0.271029  0.009002     0.525461    0.027607\n",
      "      2         low silverman   0.421302  0.029298   0.477747  0.017003     0.787618    0.031399\n",
      "      2         low        cv   0.277729  0.016931   0.383344  0.009055     0.694364    0.024631\n",
      "      2    moderate silverman   0.266158  0.019718   0.383053  0.012630     0.646683    0.024279\n",
      "      2    moderate        cv   0.254015  0.025018   0.361438  0.009228     0.653204    0.024279\n",
      "      2        high silverman   0.250130  0.026146   0.355603  0.012392     0.617052    0.029933\n",
      "      2        high        cv   0.262736  0.033502   0.352613  0.011203     0.617478    0.025880\n",
      "      3         low silverman   0.442119  0.043107   0.468577  0.017477     0.813627    0.034869\n",
      "      3         low        cv   0.357676  0.036973   0.405299  0.011311     0.729837    0.032797\n",
      "      3    moderate silverman   0.323767  0.038042   0.395698  0.013032     0.672823    0.027941\n",
      "      3    moderate        cv   0.322690  0.038028   0.387601  0.010935     0.741343    0.042098\n",
      "      3        high silverman   0.194323  0.019109   0.319166  0.009865     0.577187    0.030755\n",
      "      3        high        cv   0.245965  0.034644   0.330056  0.009655     0.565276    0.023503\n",
      "      4         low silverman   0.652899  0.077806   0.541412  0.021077     0.946020    0.042160\n",
      "      4         low        cv   0.402370  0.048842   0.419347  0.011938     0.786258    0.040426\n",
      "      4    moderate silverman   0.389731  0.034947   0.428847  0.015712     0.747400    0.030875\n",
      "      4    moderate        cv   0.331790  0.028196   0.388607  0.011089     0.712758    0.035157\n",
      "      4        high silverman   0.294748  0.041287   0.369462  0.013086     0.648222    0.034565\n",
      "      4        high        cv   0.301762  0.038942   0.374032  0.011506     0.673833    0.031226\n",
      "      5         low silverman   0.785945  0.069512   0.605627  0.021223     1.119623    0.043487\n",
      "      5         low        cv   0.554934  0.073949   0.456186  0.013589     0.859622    0.033926\n",
      "      5    moderate silverman   0.413258  0.030702   0.454040  0.014146     0.823951    0.032432\n",
      "      5    moderate        cv   0.556297  0.166393   0.418806  0.014837     0.759752    0.031685\n",
      "      5        high silverman   0.302680  0.026729   0.380320  0.011414     0.652646    0.030441\n",
      "      5        high        cv   0.358080  0.046427   0.385776  0.010903     0.761227    0.065984\n",
      "      6         low silverman   0.972095  0.073223   0.660408  0.022155     1.280631    0.051320\n",
      "      6         low        cv   0.513054  0.046652   0.460275  0.011674     0.835438    0.032351\n",
      "      6    moderate silverman   0.590936  0.079369   0.499106  0.018390     0.940200    0.042942\n",
      "      6    moderate        cv   0.453427  0.121521   0.411190  0.013356     0.821333    0.073757\n",
      "      6        high silverman   0.320134  0.034695   0.396238  0.012378     0.683497    0.026388\n",
      "      6        high        cv   0.318952  0.033518   0.383001  0.010125     0.710019    0.029110\n",
      "=====================================================================================\n",
      "\n",
      "Сохранено: kernel_regression_polynomial_results.csv\n"
     ]
    }
   ],
   "source": [
    "log_df = pd.read_csv('../datasets/synthetic/synthetic_datasets_with_coeffs/seed_log.csv')\n",
    "results = []\n",
    "\n",
    "for degree in range(1, 7):\n",
    "    for noise_level in ['low', 'moderate', 'high']:\n",
    "        df = pd.read_csv(f'../datasets/synthetic/synthetic_datasets_with_coeffs/noise_{noise_level}_deg{degree}.csv')\n",
    "        metrics = {'silverman': {'imse': [], 'imae': [], 'maxerr': []},\n",
    "                   'cv': {'imse': [], 'imae': [], 'maxerr': []}}\n",
    "        \n",
    "        for seed in df['seed'].unique()[:300]:\n",
    "            subset = df[df['seed'] == seed]\n",
    "            x_train = subset['x'].values\n",
    "            y_train = subset['y_noisy'].values\n",
    "            \n",
    "            # Истинная регрессия\n",
    "            coeffs = [subset[f'coeff_{i}'].iloc[0] for i in range(degree + 1)]\n",
    "            g_true = lambda x, c=coeffs: sum(c[i] * x**i for i in range(len(c)))\n",
    "            \n",
    "            # Модели\n",
    "            model_silverman = kernel_regression_silverman(x_train, y_train)\n",
    "            model_cv = kernel_regression_cv_safe(x_train, y_train)\n",
    "            \n",
    "            # Callable для метрик\n",
    "            g_silverman = lambda x, m=model_silverman: m.fit(np.array([[x]]))[0][0]\n",
    "            g_cv = lambda x, m=model_cv: m.fit(np.array([[x]]))[0][0]\n",
    "            \n",
    "            # Расчёт метрик (интегральные + дискретные для валидации)\n",
    "            for method, g_hat in [('silverman', g_silverman), ('cv', g_cv)]:\n",
    "                try:\n",
    "                    metrics[method]['imse'].append(imse(g_hat, g_true))\n",
    "                    metrics[method]['imae'].append(imae(g_hat, g_true))\n",
    "                    metrics[method]['maxerr'].append(maxerr(g_hat, g_true))\n",
    "                except:\n",
    "                    # Фолбэк на дискретные метрики при сбое интегрирования\n",
    "                    metrics[method]['imse'].append(imse_discrete(g_hat, g_true))\n",
    "                    metrics[method]['imae'].append(imae_discrete(g_hat, g_true))\n",
    "                    metrics[method]['maxerr'].append(maxerr_discrete(g_hat, g_true))\n",
    "        \n",
    "        # Агрегация результатов\n",
    "        for method in ['silverman', 'cv']:\n",
    "            m = metrics[method]\n",
    "            results.append({\n",
    "                'degree': degree,\n",
    "                'noise_level': noise_level,\n",
    "                'method': method,\n",
    "                'imse_mean': np.mean(m['imse']),\n",
    "                'imse_sem': np.std(m['imse']) / np.sqrt(len(m['imse'])),  # ← SEM вместо σ\n",
    "                'imae_mean': np.mean(m['imae']),\n",
    "                'imae_sem': np.std(m['imae']) / np.sqrt(len(m['imae'])),\n",
    "                'maxerr_mean': np.mean(m['maxerr']),\n",
    "                'maxerr_sem': np.std(m['maxerr']) / np.sqrt(len(m['maxerr']))\n",
    "            })\n",
    "\n",
    "# Сохранение и вывод\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('kernel_regression_polynomial_results.csv', index=False)\n",
    "\n",
    "print(\"Результаты ядерной регрессии на полиномиальных данных\")\n",
    "print(\"=\" * 85)\n",
    "print(results_df.to_string(index=False, float_format='%.6f'))\n",
    "print(\"=\" * 85)\n",
    "print(f\"\\nСохранено: kernel_regression_polynomial_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76feb786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M A C H R E A T O R\\AppData\\Local\\Temp\\ipykernel_11268\\2511819131.py:50: IntegrationWarning: The maximum number of subdivisions (100) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=100)\n",
      "C:\\Users\\M A C H R E A T O R\\AppData\\Local\\Temp\\ipykernel_11268\\2511819131.py:50: IntegrationWarning: The maximum number of subdivisions (100) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Функции ядерной регрессии\n",
    "def kernel_regression_silverman(x_train, y_train):\n",
    "    x_train = np.asarray(x_train).reshape(-1, 1)\n",
    "    h = 1.06 * np.std(x_train) * len(x_train)**(-1/5)\n",
    "    return KernelReg(y_train, x_train, var_type='c', bw=[h])\n",
    "\n",
    "def kernel_regression_cv_safe(x_train, y_train):\n",
    "    x_train = np.asarray(x_train).reshape(-1, 1)\n",
    "    h_silverman = 1.06 * np.std(x_train) * len(x_train)**(-1/5)\n",
    "    \n",
    "    kf = KFold(5)\n",
    "    \n",
    "    def cv_mse(log_h):\n",
    "        h = np.exp(log_h)\n",
    "        # Защита от экстремальных значений\n",
    "        if h < 1e-6 or h > 100.0 or np.isnan(h) or np.isinf(h):\n",
    "            return 1e10\n",
    "        \n",
    "        total_mse = 0.0\n",
    "        try:\n",
    "            for train_idx, val_idx in kf.split(x_train):\n",
    "                kr = KernelReg(y_train[train_idx], x_train[train_idx], var_type='c', bw=[h])\n",
    "                pred = kr.fit(x_train[val_idx])[0]\n",
    "                total_mse += np.mean((pred - y_train[val_idx])**2)\n",
    "            return total_mse / kf.n_splits\n",
    "        except:\n",
    "            return 1e10\n",
    "    \n",
    "    h_init = np.log(h_silverman)\n",
    "    res = minimize(cv_mse, x0=h_init, method='L-BFGS-B')\n",
    "    best_h = np.exp(res.x[0]) if res.success and not (np.isnan(res.x[0]) or np.isinf(res.x[0])) else h_silverman\n",
    "    \n",
    "    return KernelReg(y_train, x_train, var_type='c', bw=[best_h])\n",
    "\n",
    "# Интегральные метрики\n",
    "def imse(g_hat, g_true, a=0, b=1, epsabs=1e-6):\n",
    "    integrand = lambda x: (g_hat(x) - g_true(x)) ** 2\n",
    "    integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=100)\n",
    "    return integral / (b - a)\n",
    "\n",
    "def imae(g_hat, g_true, a=0, b=1, epsabs=1e-6):\n",
    "    integrand = lambda x: abs(g_hat(x) - g_true(x))\n",
    "    integral, _ = quad(integrand, a, b, epsabs=epsabs, limit=100)\n",
    "    return integral / (b - a)\n",
    "\n",
    "def maxerr(g_hat, g_true, a=0, b=1, xatol=1e-6):\n",
    "    objective = lambda x: -abs(g_hat(x) - g_true(x))\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded', options={'xatol': xatol})\n",
    "    if result.success:\n",
    "        return -result.fun\n",
    "    x_grid = np.linspace(a, b, 500)\n",
    "    return max(abs(g_hat(x) - g_true(x)) for x in x_grid)\n",
    "\n",
    "# Дискретные метрики (для валидации)\n",
    "def imse_discrete(g_hat, g_true, a=0, b=1, n_points=1000):\n",
    "    x_grid = np.linspace(a, b, n_points)\n",
    "    errors = [(g_hat(x) - g_true(x)) ** 2 for x in x_grid]\n",
    "    return np.mean(errors)\n",
    "\n",
    "def imae_discrete(g_hat, g_true, a=0, b=1, n_points=1000):\n",
    "    x_grid = np.linspace(a, b, n_points)\n",
    "    errors = [abs(g_hat(x) - g_true(x)) for x in x_grid]\n",
    "    return np.mean(errors)\n",
    "\n",
    "def maxerr_discrete(g_hat, g_true, a=0, b=1, n_points=1000):\n",
    "    x_grid = np.linspace(a, b, n_points)\n",
    "    errors = [abs(g_hat(x) - g_true(x)) for x in x_grid]\n",
    "    return max(errors)\n",
    "\n",
    "# Тестирование с параллельным расчётом метрик\n",
    "log_df = pd.read_csv('../datasets/synthetic/synthetic_datasets_with_coeffs/seed_log.csv')\n",
    "results_integral = []\n",
    "results_discrete = []\n",
    "\n",
    "for degree in range(1, 7):\n",
    "    for noise_level in ['low', 'moderate', 'high']:\n",
    "        df = pd.read_csv(f'../datasets/synthetic/synthetic_datasets_with_coeffs/noise_{noise_level}_deg{degree}.csv')\n",
    "        \n",
    "        # Контейнеры для метрик\n",
    "        metrics_int = {'silverman': {'imse': [], 'imae': [], 'maxerr': []},\n",
    "                       'cv': {'imse': [], 'imae': [], 'maxerr': []}}\n",
    "        metrics_disc = {'silverman': {'imse': [], 'imae': [], 'maxerr': []},\n",
    "                        'cv': {'imse': [], 'imae': [], 'maxerr': []}}\n",
    "        \n",
    "        for seed in df['seed'].unique()[:30]:\n",
    "            subset = df[df['seed'] == seed]\n",
    "            x_train = subset['x'].values\n",
    "            y_train = subset['y_noisy'].values\n",
    "            \n",
    "            # Истинная регрессия из коэффициентов\n",
    "            coeffs = [subset[f'coeff_{i}'].iloc[0] for i in range(degree + 1)]\n",
    "            g_true = lambda x, c=coeffs: sum(c[i] * x**i for i in range(len(c)))\n",
    "            \n",
    "            # Обучение моделей\n",
    "            model_silverman = kernel_regression_silverman(x_train, y_train)\n",
    "            model_cv = kernel_regression_cv_safe(x_train, y_train)\n",
    "            \n",
    "            # Callable функции\n",
    "            g_silverman = lambda x, m=model_silverman: m.fit(np.array([[x]]))[0][0]\n",
    "            g_cv = lambda x, m=model_cv: m.fit(np.array([[x]]))[0][0]\n",
    "            \n",
    "            # Расчёт ОБЕИХ версий метрик\n",
    "            for method, g_hat in [('silverman', g_silverman), ('cv', g_cv)]:\n",
    "                # Интегральные метрики\n",
    "                metrics_int[method]['imse'].append(imse(g_hat, g_true))\n",
    "                metrics_int[method]['imae'].append(imae(g_hat, g_true))\n",
    "                metrics_int[method]['maxerr'].append(maxerr(g_hat, g_true))\n",
    "                \n",
    "                # Дискретные метрики\n",
    "                metrics_disc[method]['imse'].append(imse_discrete(g_hat, g_true))\n",
    "                metrics_disc[method]['imae'].append(imae_discrete(g_hat, g_true))\n",
    "                metrics_disc[method]['maxerr'].append(maxerr_discrete(g_hat, g_true))\n",
    "        \n",
    "        # Агрегация результатов для интегральных метрик\n",
    "        for method in ['silverman', 'cv']:\n",
    "            m = metrics_int[method]\n",
    "            results_integral.append({\n",
    "                'degree': degree,\n",
    "                'noise_level': noise_level,\n",
    "                'method': method,\n",
    "                'imse_mean': np.mean(m['imse']),\n",
    "                'imse_std': np.std(m['imse']),\n",
    "                'imae_mean': np.mean(m['imae']),\n",
    "                'imae_std': np.std(m['imae']),\n",
    "                'maxerr_mean': np.mean(m['maxerr']),\n",
    "                'maxerr_std': np.std(m['maxerr'])\n",
    "            })\n",
    "        \n",
    "        # Агрегация результатов для дискретных метрик\n",
    "        for method in ['silverman', 'cv']:\n",
    "            m = metrics_disc[method]\n",
    "            results_discrete.append({\n",
    "                'degree': degree,\n",
    "                'noise_level': noise_level,\n",
    "                'method': method,\n",
    "                'imse_mean': np.mean(m['imse']),\n",
    "                'imse_std': np.std(m['imse']),\n",
    "                'imae_mean': np.mean(m['imae']),\n",
    "                'imae_std': np.std(m['imae']),\n",
    "                'maxerr_mean': np.mean(m['maxerr']),\n",
    "                'maxerr_std': np.std(m['maxerr'])\n",
    "            })\n",
    "\n",
    "# Сохранение результатов\n",
    "df_integral = pd.DataFrame(results_integral)\n",
    "df_discrete = pd.DataFrame(results_discrete)\n",
    "\n",
    "df_integral.to_csv('kernel_regression_integral.csv', index=False)\n",
    "df_discrete.to_csv('kernel_regression_discrete.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0477fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "СРАВНЕНИЕ ИНТЕГРАЛЬНЫХ И ДИСКРЕТНЫХ МЕТРИК\n",
      "===============================================================================================\n",
      "Степень  Шум        Метод        IMSE (инт)      IMSE (дискр)    Отклонение %   \n",
      "-----------------------------------------------------------------------------------------------\n",
      "1        low        silverman    0.162036        0.162341        0.19           \n",
      "1        low        cv           1.435334        1.434366        0.07           \n",
      "1        moderate   silverman    0.155697        0.156020        0.21           \n",
      "1        moderate   cv           0.133213        0.133569        0.27           \n",
      "1        high       silverman    0.129291        0.129669        0.29           \n",
      "1        high       cv           0.179930        0.181281        0.75           \n",
      "2        low        silverman    0.391991        0.392342        0.09           \n",
      "2        low        cv           0.235423        0.235919        0.21           \n",
      "2        moderate   silverman    0.285258        0.285774        0.18           \n",
      "2        moderate   cv           0.342520        0.344083        0.45           \n",
      "2        high       silverman    0.228880        0.229153        0.12           \n",
      "2        high       cv           0.283512        0.283796        0.10           \n",
      "3        low        silverman    0.353572        0.354258        0.19           \n",
      "3        low        cv           0.325765        0.326809        0.32           \n",
      "3        moderate   silverman    0.299885        0.300342        0.15           \n",
      "3        moderate   cv           0.278349        0.279143        0.28           \n",
      "3        high       silverman    0.231458        0.232112        0.28           \n",
      "3        high       cv           0.180931        0.181359        0.24           \n",
      "4        low        silverman    0.702141        0.703344        0.17           \n",
      "4        low        cv           0.632211        0.639304        1.11           \n",
      "4        moderate   silverman    0.663907        0.667299        0.51           \n",
      "4        moderate   cv           1.022045        1.036832        1.43           \n",
      "4        high       silverman    0.224543        0.224840        0.13           \n",
      "4        high       cv           0.231150        0.231547        0.17           \n",
      "5        low        silverman    0.888779        0.890296        0.17           \n",
      "5        low        cv           0.346810        0.348202        0.40           \n",
      "5        moderate   silverman    0.372088        0.374056        0.53           \n",
      "5        moderate   cv           0.311090        0.313072        0.63           \n",
      "5        high       silverman    0.328219        0.330256        0.62           \n",
      "5        high       cv           0.310519        0.312587        0.66           \n",
      "6        low        silverman    0.798662        0.802727        0.51           \n",
      "6        low        cv           0.681742        0.686321        0.67           \n",
      "6        moderate   silverman    0.499908        0.502288        0.47           \n",
      "6        moderate   cv           0.329084        0.330833        0.53           \n",
      "6        high       silverman    0.282694        0.283607        0.32           \n",
      "6        high       cv           0.283395        0.284628        0.43           \n",
      "===============================================================================================\n",
      "\n",
      "Сохранено:\n",
      "  kernel_regression_integral.csv   — интегральные метрики\n",
      "  kernel_regression_discrete.csv   — дискретные метрики\n",
      "\n",
      "Рекомендация: если отклонение < 1%, интегральные метрики можно считать корректными.\n"
     ]
    }
   ],
   "source": [
    "# Вывод сравнения для нескольких комбинаций\n",
    "print(\"СРАВНЕНИЕ ИНТЕГРАЛЬНЫХ И ДИСКРЕТНЫХ МЕТРИК\")\n",
    "print(\"=\" * 95)\n",
    "print(f\"{'Степень':<8} {'Шум':<10} {'Метод':<12} {'IMSE (инт)':<15} {'IMSE (дискр)':<15} {'Отклонение %':<15}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for i in range(0, len(df_integral)):  # Выводим по 2 метода на комбинацию (степень+шум)\n",
    "    row_int = df_integral.iloc[i]\n",
    "    row_disc = df_discrete.iloc[i]\n",
    "    rel_diff = abs(row_int['imse_mean'] - row_disc['imse_mean']) / row_disc['imse_mean'] * 100\n",
    "    \n",
    "    print(f\"{row_int['degree']:<8} {row_int['noise_level']:<10} {row_int['method']:<12} \"\n",
    "          f\"{row_int['imse_mean']:<15.6f} {row_disc['imse_mean']:<15.6f} {rel_diff:<15.2f}\")\n",
    "\n",
    "print(\"=\" * 95)\n",
    "print(\"\\nСохранено:\")\n",
    "print(\"  kernel_regression_integral.csv   — интегральные метрики\")\n",
    "print(\"  kernel_regression_discrete.csv   — дискретные метрики\")\n",
    "print(\"\\nРекомендация: если отклонение < 1%, интегральные метрики можно считать корректными.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6378c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      " degree noise_level    method  imse_mean  imse_std  imae_mean  imae_std  maxerr_mean  maxerr_std\n",
      "      1         low silverman   0.162036  0.127692   0.303220  0.126002     0.622027    0.310575\n",
      "      1         low        cv   1.435334  7.048368   0.342159  0.385613     1.976011    7.931115\n",
      "      1    moderate silverman   0.155697  0.161374   0.291610  0.138797     0.504707    0.440628\n",
      "      1    moderate        cv   0.133213  0.152265   0.261826  0.136334     0.447013    0.419772\n",
      "      1        high silverman   0.129291  0.100534   0.263552  0.101587     0.434250    0.238610\n",
      "      1        high        cv   0.179930  0.408481   0.262964  0.154412     0.422804    0.228981\n",
      "      2         low silverman   0.391991  0.388919   0.483569  0.258437     0.822938    0.466523\n",
      "      2         low        cv   0.235423  0.158412   0.372275  0.124581     0.639168    0.266430\n",
      "      2    moderate silverman   0.285258  0.385275   0.359416  0.242443     0.557338    0.426036\n",
      "      2    moderate        cv   0.342520  0.875305   0.341626  0.286358     0.575165    0.415220\n",
      "      2        high silverman   0.228880  0.196105   0.365723  0.171676     0.673253    0.444581\n",
      "      2        high        cv   0.283512  0.389521   0.375165  0.157866     0.673155    0.517022\n",
      "      3         low silverman   0.353572  0.454919   0.444498  0.255631     0.748156    0.409465\n",
      "      3         low        cv   0.325765  0.410540   0.410881  0.195163     0.637500    0.288404\n",
      "      3    moderate silverman   0.299885  0.312151   0.417006  0.209499     0.694077    0.430534\n",
      "      3    moderate        cv   0.278349  0.244214   0.400969  0.178800     0.653355    0.306798\n",
      "      3        high silverman   0.231458  0.322821   0.350321  0.178775     0.628119    0.377843\n",
      "      3        high        cv   0.180931  0.092422   0.329753  0.110044     0.626879    0.331656\n",
      "      4         low silverman   0.702141  1.067366   0.534006  0.396750     0.962077    0.909345\n",
      "      4         low        cv   0.632211  1.458351   0.439201  0.275275     0.747008    0.607029\n",
      "      4    moderate silverman   0.663907  0.995693   0.502762  0.327545     0.835771    0.603952\n",
      "      4    moderate        cv   1.022045  3.764069   0.429993  0.256194     0.765011    0.408544\n",
      "      4        high silverman   0.224543  0.203193   0.376279  0.155599     0.630442    0.361921\n",
      "      4        high        cv   0.231150  0.209954   0.363628  0.158553     0.682997    0.425831\n",
      "      5         low silverman   0.888779  1.465598   0.661426  0.396217     1.277517    0.880016\n",
      "      5         low        cv   0.346810  0.218289   0.444787  0.140426     0.927013    0.479138\n",
      "      5    moderate silverman   0.372088  0.413926   0.410255  0.219440     0.578904    0.334326\n",
      "      5    moderate        cv   0.311090  0.288349   0.367716  0.169121     0.638486    0.323782\n",
      "      5        high silverman   0.328219  0.887373   0.340540  0.180518     0.571275    0.276515\n",
      "      5        high        cv   0.310519  0.794212   0.332618  0.158363     0.601886    0.327526\n",
      "      6         low silverman   0.798662  1.511696   0.570514  0.299668     0.931998    0.549290\n",
      "      6         low        cv   0.681742  1.411923   0.488571  0.220476     0.836952    0.482513\n",
      "      6    moderate silverman   0.499908  0.593320   0.486233  0.262931     0.862015    0.519603\n",
      "      6    moderate        cv   0.329084  0.370448   0.400370  0.190286     0.716515    0.402476\n",
      "      6        high silverman   0.282694  0.314133   0.367093  0.186057     0.774055    0.942446\n",
      "      6        high        cv   0.283395  0.243216   0.364237  0.134923     0.641374    0.447355\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 85)\n",
    "print(df_integral.to_string(index=False, float_format='%.6f'))\n",
    "print(\"=\" * 85)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
